# app/ingest.py
import os
from pathlib import Path
from tqdm import tqdm
from rich import print as rprint
import traceback
import pandas as pd
from app.config import CHROMA_DB_PATH
from app.utils import extract_text
from app.embedd_manifest import load_manifest, save_manifest

# LangChain setup
os.environ["LANGCHAIN_ENDPOINT"] = "none"

from langchain_chroma import Chroma
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings


def ingest_documents():
    folder = Path("data/raw_documents")
    if not folder.exists():
        rprint("[red]‚ùå Folder data/raw_documents does not exist.[/]")
        return

    files = list(folder.glob("*"))
    if not files:
        rprint("[red]‚ùå No files found in data/raw_documents/[/]")
        return

    rprint(f"[bold blue]üìÇ Found {len(files)} files in data/raw_documents/[/]")

    embedded_files = load_manifest()
    rprint(f"[yellow]üìú Previously embedded: {len(embedded_files)} files[/]")

    docs = []
    newly_embedded_files = set()
    total_chars = 0

    rprint("\n[bold white]üìÑ Processing documents...[/]")

    for f in tqdm(files, desc="Reading and chunking files"):
        if f.name in embedded_files:
            rprint(f"[dim]‚è≠Ô∏è Skipping {f.name} ‚Äî already embedded.[/]")
            continue

        if f.suffix == ".xlsx":
            try:
                df = pd.read_excel(f)
                df.columns = [col.strip().lower().replace(" ", "_") for col in df.columns]
                
                for i, row in df.iterrows():
                    product_text = f"""
                    Product Name: {row.get('product_name', '')}
                    Brand: {row.get('brand_name', '')}
                    Key Ingredients: {row.get('key_ingredients', '')}
                    All Ingredients: {row.get('all_ingredients', '')}
                    MRP: ‚Çπ{row.get('mrp', '')}
                    Description: {row.get('product_description', '')}
                    Features & Benefits: {row.get('key_features_&_benefits', '')}
                    How To Use: {row.get('how_to_use', '')}
                    About Brand: {row.get('about_the_brand', '')}
                    Age Suitability: {row.get('age', '')}
                    Skin Type: {row.get('skin_type', '')}
                    Hair Type: {row.get('hair_type', '')}
                    Skin Tone: {row.get('skin_tone', '')}
                    SPF: {row.get('spf', '')}
                    Super Ingredients: {row.get('super_ingredients', '')}
                    Benefits: {row.get('benefits', '')}
                    Fragrance Family: {row.get('fragrance_family', '')}
                    Makeup Finish: {row.get('make_up_finish', '')}
                    Dimensions: {row.get('dimensions', '')}
                    Imported By: {row.get('imported_by', '')}
                    """

                    docs.append(Document(page_content=product_text.strip(), metadata={"source": f.name}))
                
                newly_embedded_files.add(f.name)
                rprint(f"[green]üìò {f.name} ‚Äî embedded {len(df)} Excel rows[/]")
                continue  # Skip default chunking/extract for Excel

            except Exception as e:
                rprint(f"[red]‚ùå Failed to process Excel file {f.name}: {e}[/]")
                traceback.print_exc()
                continue

        try:
            text = extract_text(f)
            if not text.strip():
                rprint(f"[yellow]‚ö†Ô∏è Skipping {f.name} ‚Äî empty or unreadable.[/]")
                continue

            total_chars += len(text)
            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=400)
            chunks = splitter.split_text(text)

            for chunk in chunks:
                docs.append(Document(page_content=chunk, metadata={"source": f.name}))

            newly_embedded_files.add(f.name)
            rprint(f"[green]üìÑ {f.name} ‚Äî {len(chunks)} chunks | {len(text)} chars[/]")

        except Exception as e:
            rprint(f"[red]‚ùå Error processing {f.name}: {e}[/]")
            traceback.print_exc()

    if not docs:
        rprint("[red]‚ùå No new documents to embed.[/]")
        return

    rprint(f"\n‚úÖ Total characters processed: {total_chars}")
    rprint(f"‚úÖ Total new chunks to embed: {len(docs)}")

    # Load embedding model
    rprint("\n[bold]üîó Loading embedding model...[/]")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"
    )


    rprint("[yellow]üí° Creating embeddings...[/]")
    texts = [doc.page_content for doc in docs]
    batch_size = 100

    # embeddings are created here but not directly used; can be omitted if unnecessary
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding chunks"):
        batch = texts[i:i + batch_size]
        _ = embedding_model.embed_documents(batch)  # call to warm up or generate embeddings if needed

    # Remove any existing 'embedding' from metadata to avoid Chroma errors
    for doc in docs:
        if "embedding" in doc.metadata:
            del doc.metadata["embedding"]

    # Save documents to Chroma vectorstore
    rprint(f"\n[bold cyan]üíæ Saving to Chroma vectorstore at: {Path(CHROMA_DB_PATH).resolve()}[/]")
    try:
        vectorstore = Chroma(
            persist_directory=CHROMA_DB_PATH,
            embedding_function=embedding_model
        )

        for i in tqdm(range(0, len(docs), batch_size), desc="Saving to vectorstore"):
            batch_docs = docs[i:i + batch_size]
            vectorstore.add_documents(batch_docs)

        # No explicit persist() call needed ‚Äî auto-persist enabled by persist_directory
        rprint(f"‚úÖ Saved {len(docs)} chunks to ChromaDB.")
    except Exception as e:
        rprint(f"[red]‚ùå Failed to save to Chroma: {e}[/]")
        return

    # Print vector count
    try:
        count = vectorstore._collection.count()
        rprint(f"‚ÑπÔ∏è Vectorstore now contains {count} vectors.")
    except Exception as e:
        rprint(f"[yellow]‚ö†Ô∏è Could not get vectorstore count: {e}[/]")

    # Update manifest file with newly embedded files
    embedded_files.update(newly_embedded_files)
    save_manifest(embedded_files)
    rprint(f"[bold green]üìù Updated embed manifest with {len(newly_embedded_files)} new files.[/]")


if __name__ == "__main__":
    ingest_documents()
